{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import click, torch, logging\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def init_logger():\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s %(levelname)-8s | %(message)s\",\n",
    "        level=logging.INFO,\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    "init_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-23 09:03:57 INFO     | Loading tokenizer & model..\n",
      "2023-12-23 09:04:13 INFO     | Moving model to cuda:0..\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "\n",
    "MODEL_NAME = \"bigscience/bloom-1b1\"\n",
    "# MODEL_NAME = \"bigscience/bloom-7b1\"\n",
    "# MODEL_NAME = \"lmsys/vicuna-7b-v1.3\"\n",
    "\n",
    "if device == 'cpu':\n",
    "    logging.warning(\"Running on CPU!! Operations may be slow\")\n",
    "\n",
    "logging.info(\"Loading tokenizer & model..\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "logging.info(f\"Moving model to {device}..\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "for filename in os.listdir(\"mgsm\"):\n",
    "    path = f\"mgsm/{filename}\"\n",
    "    newpath = path.split(\".tsv\")[0] + \".csv\"\n",
    "    df = pd.read_csv(path, sep=\"\\t\", index_col=False, names=['query', 'label'])\n",
    "    df.to_csv(newpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(prompt: str, max_len: int = 10):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        generate_ids = model.generate(inputs.input_ids.to(device), max_length=max_len)\n",
    "        decoding = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "        print(decoding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flowers are the most common cause of death in patients with acute coronary syndromes. The incidence of coronary artery disease is increasing in the United States, and the number of patients with acute coronary syndromes is expected to increase in the future. The incidence of coronary artery disease is increasing in the United States, and the number of patients with acute coronary syndromes is expected to increase in the future. The incidence of coronary artery disease is increasing in the United States, and the number of patients with acute coronary\n"
     ]
    }
   ],
   "source": [
    "run_prompt(\"flowers are\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usually, When people are sick, they are not able to work. They are not able to do their jobs. They are not able to do their work. They are not able to do their work. They are not able to do their work. They are not able to do their work. They are not able to do their work. They are not able to do their work. They are not able to do their work. They are not able to do their work. They are not able\n"
     ]
    }
   ],
   "source": [
    "run_prompt(\"Usually, When people are sick,\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bees, and the\n",
      "bees are the only ones who can do it.\"\n",
      "\n",
      "\"Then you are the only one who can do it,\" said the old man, with a\n",
      "smile.\n",
      "\n",
      "\"Then I am the only one\n"
     ]
    }
   ],
   "source": [
    "run_prompt(\"Bees\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is 1+1?\");\n",
      "        }\n",
      "        else if (isNumber(value)) {\n"
     ]
    }
   ],
   "source": [
    "run_prompt(\"what is 1+1?\", 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les canes de Janet pondent 16 œufs par jour. Chaque matin, elle en mange trois au petit déjeuner et en utilise quatre autres pour préparer des muffins pour ses amis. Ce qui reste, elle le vend quotidiennement au marché fermier, au prix de 2 $ l'œuf de cane frais. Combien (en dollars) gagne-t-elle chaque jour au marché fermier ?\n"
     ]
    }
   ],
   "source": [
    "run_prompt(\"Les canes de Janet pondent 16 œufs par jour. Chaque matin, elle en mange trois au petit déjeuner et en utilise quatre autres pour préparer des muffins pour ses amis. Ce qui reste, elle le vend quotidiennement au marché fermier, au prix de 2 $ l'œuf de cane frais. Combien (en dollars) gagne-t-elle chaque jour au marché fermier ?\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
